{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from numpy.random import gamma, binomial\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import xarray as xr \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from convCNP.validation.utils import get_dists\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xarray as xr\n",
    "\n",
    "from convCNP.models.elev_models import TmaxBiasConvCNPElev, GammaBiasConvCNPElev\n",
    "from convCNP.models.cnn import CNN, ResConvBlock\n",
    "from convCNP.training.training_elev import train_elev\n",
    "from convCNP.training.loss_functions import gll, gamma_ll\n",
    "from convCNP.training.utils import get_value_tmax\n",
    "from convCNP.validation.utils import get_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and mode parameters\n",
    "VARIABLE = 'tmax'   # 'tmax' or 'precip'\n",
    "DATA_YEAR_START = 2023  # Year to start loading data from, None to include all data\n",
    "                        # Used for quicker testing\n",
    "\n",
    "# Model parameters\n",
    "N_CHANNELS = 128    # default in paper is 128\n",
    "N_BLOCKS = 6        # default in paper is 6\n",
    "KERNEL_SIZE = 5     # default in paper is 5\n",
    "LENGTH_SCALE = 0.1  # default in paper is 0.1\n",
    "IN_CHANNELS = 25    # default in paper is 25\n",
    "\n",
    "# Training parameters\n",
    "N_EPOCHS = 100      # default in paper is 100\n",
    "BATCH_SIZE = 16     # default in paper is 16    # TODO: wire this in\n",
    "LR = 5e-4           # default in paper is 5e-4\n",
    "PATIENCE = 10       # default in paper is 10    # TODO: wire this in\n",
    "\n",
    "# Cross-validation parameters\n",
    "N_FOLDS = 5         # default in paper is 5\n",
    "\n",
    "# Other parameters\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'trained_models/'\n",
    "\n",
    "# Auto-set data paths based on variable\n",
    "if VARIABLE == 'tmax':\n",
    "    GRID_INPUTS, GRID_TARGETS = f'./datasets/ERA5_Land/max_temperature/*.nc', f'./datasets/EOBS/max_temperature/*.nc'\n",
    "elif VARIABLE == 'precip':\n",
    "    GRID_INPUTS, GRID_TARGETS = f'./datasets/ERA5_Land/precipitation/*.nc', f'./datasets/EOBS/precipitation/*.nc'\n",
    "ELEV_INPUTS = ''  # Empty for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " <xarray.Dataset> Size: 166MB\n",
      "Dimensions:    (time: 23376, latitude: 29, longitude: 61)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 187kB 1960-01-01 1960-01-02 ... 2023-12-31\n",
      "  * latitude   (latitude) float64 232B 48.2 48.1 48.0 47.9 ... 45.6 45.5 45.4\n",
      "  * longitude  (longitude) float64 488B 5.0 5.1 5.2 5.3 ... 10.7 10.8 10.9 11.0\n",
      "Data variables:\n",
      "    t2m_max    (time, latitude, longitude) float32 165MB dask.array<chunksize=(366, 29, 61), meta=np.ndarray>\n",
      "\n",
      "Target data:\n",
      " <xarray.Dataset> Size: 122MB\n",
      "Dimensions:    (time: 19540, latitude: 30, longitude: 52)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 156kB 1971-01-01 1971-01-02 ... 2024-06-30\n",
      "  * latitude   (latitude) float64 240B 45.35 45.45 45.55 ... 48.05 48.15 48.25\n",
      "  * longitude  (longitude) float64 416B 5.65 5.75 5.85 ... 10.55 10.65 10.75\n",
      "Data variables:\n",
      "    t_max      (time, latitude, longitude) float32 122MB dask.array<chunksize=(1, 30, 52), meta=np.ndarray>\n",
      "Attributes:\n",
      "    CDI:            Climate Data Interface version 1.9.6 (http://mpimet.mpg.d...\n",
      "    history:        Tue Jul 22 14:45:22 2025: cdo -chname,tx,t_max -sellonlat...\n",
      "    Conventions:    CF-1.4\n",
      "    E-OBS_version:  30.0e\n",
      "    References:     http://surfobs.climate.copernicus.eu/dataaccess/access_eo...\n",
      "    NCO:            netCDF Operators version 5.1.8 (Homepage = http://nco.sf....\n",
      "    CDO:            Climate Data Operators version 1.9.6 (http://mpimet.mpg.d...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "full_input_ds = xr.open_mfdataset(GRID_INPUTS, combine='by_coords')\n",
    "full_target_ds = xr.open_mfdataset(GRID_TARGETS, combine='by_coords')\n",
    "\n",
    "print(\"Input data:\\n\", full_input_ds)\n",
    "print(\"\\nTarget data:\\n\", full_target_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data xarray:\n",
      " <xarray.Dataset> Size: 88MB\n",
      "Dimensions:    (time: 12418, latitude: 29, longitude: 61)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 99kB 1990-01-01 1990-01-02 ... 2023-12-31\n",
      "  * latitude   (latitude) float64 232B 48.2 48.1 48.0 47.9 ... 45.6 45.5 45.4\n",
      "  * longitude  (longitude) float64 488B 5.0 5.1 5.2 5.3 ... 10.7 10.8 10.9 11.0\n",
      "Data variables:\n",
      "    t2m_max    (time, latitude, longitude) float32 88MB dask.array<chunksize=(365, 29, 61), meta=np.ndarray>\n",
      "\n",
      "Target data xarray:\n",
      " <xarray.Dataset> Size: 79MB\n",
      "Dimensions:    (time: 12600, latitude: 30, longitude: 52)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 101kB 1990-01-01 1990-01-02 ... 2024-06-30\n",
      "  * latitude   (latitude) float64 240B 45.35 45.45 45.55 ... 48.05 48.15 48.25\n",
      "  * longitude  (longitude) float64 416B 5.65 5.75 5.85 ... 10.55 10.65 10.75\n",
      "Data variables:\n",
      "    t_max      (time, latitude, longitude) float32 79MB dask.array<chunksize=(1, 30, 52), meta=np.ndarray>\n",
      "Attributes:\n",
      "    CDI:            Climate Data Interface version 1.9.6 (http://mpimet.mpg.d...\n",
      "    history:        Tue Jul 22 14:45:22 2025: cdo -chname,tx,t_max -sellonlat...\n",
      "    Conventions:    CF-1.4\n",
      "    E-OBS_version:  30.0e\n",
      "    References:     http://surfobs.climate.copernicus.eu/dataaccess/access_eo...\n",
      "    NCO:            netCDF Operators version 5.1.8 (Homepage = http://nco.sf....\n",
      "    CDO:            Climate Data Operators version 1.9.6 (http://mpimet.mpg.d...\n"
     ]
    }
   ],
   "source": [
    "# Select data from YEAR_START onwards\n",
    "if DATA_YEAR_START is not None:\n",
    "    input_ds = full_input_ds.sel(time=slice(str(DATA_YEAR_START), None))\n",
    "    target_ds = full_target_ds.sel(time=slice(str(DATA_YEAR_START), None))\n",
    "else:\n",
    "    input_ds = full_input_ds\n",
    "    target_ds = full_target_ds\n",
    "\n",
    "print(\"Input data xarray:\\n\", input_ds)\n",
    "print(\"\\nTarget data xarray:\\n\", target_ds)\n",
    "\n",
    "# create a torch tensor with dimensions (time, lat, long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dists(era5_inputs, eobs_targets):\n",
    "    \"\"\"\n",
    "    Get the distances between the grid points and true points.\n",
    "    \"\"\"\n",
    "    era5_long_grid, era5_lat_grid = np.meshgrid(era5_inputs['longitude'], era5_inputs['latitude'])\n",
    "    era5_lat_grid = torch.from_numpy(era5_lat_grid).to(device)\n",
    "    era5_long_grid = torch.from_numpy(era5_long_grid).to(device)\n",
    "\n",
    "    eobs_coords = eobs_targets.stack(coords=['latitude', 'longitude'])['coords'].values\n",
    "\n",
    "    return get_dists(eobs_coords, era5_lat_grid, era5_long_grid)\n",
    "\n",
    "dists = calculate_dists(input_ds, target_ds).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(variable, n_channels, n_blocks, kernel_size, length_scale, in_channels):\n",
    "    \"\"\"\n",
    "    Build convCNP model based on variable type\n",
    "\n",
    "    Returns:\n",
    "        model: nn.Module\n",
    "        loss_fn: function\n",
    "        get_value_fn: function to extract predictions from model output\n",
    "    \"\"\"\n",
    "    # Build CNN decoder (same architecture for both variables)\n",
    "    decoder = CNN(\n",
    "        n_channels=n_channels,\n",
    "        ConvBlock=ResConvBlock,\n",
    "        n_blocks=n_blocks,\n",
    "        Conv=nn.Conv2d,\n",
    "        Normalization=nn.Identity,\n",
    "        kernel_size=kernel_size\n",
    "    )\n",
    "\n",
    "    # Build model based on variable\n",
    "    if variable == 'tmax':\n",
    "        print(\"Building TmaxBiasConvCNPElev model...\")\n",
    "        model = TmaxBiasConvCNPElev(\n",
    "            decoder=decoder,\n",
    "            in_channels=in_channels,\n",
    "            ls=length_scale\n",
    "        )\n",
    "        loss_fn = gll\n",
    "        get_value_fn = get_value_tmax\n",
    "\n",
    "    elif variable == 'precip':\n",
    "        print(\"Building GammaBiasConvCNPElev model...\")\n",
    "        model = GammaBiasConvCNPElev(\n",
    "            decoder=decoder,\n",
    "            in_channels=in_channels,\n",
    "            ls=length_scale\n",
    "        )\n",
    "        loss_fn = gamma_ll\n",
    "        # For precipitation, we need a custom function\n",
    "        def get_value_precip(p):\n",
    "            \"\"\"Extract mean prediction for precipitation\"\"\"\n",
    "            # For Gamma distribution, mean = alpha / beta\n",
    "            # Only predict non-zero when rho > 0.5\n",
    "            mean = p[:, :, 1] / p[:, :, 2]  # alpha / beta\n",
    "            mean[p[:, :, 0] <= 0.5] = 0  # Set to 0 when dry\n",
    "            return mean\n",
    "        get_value_fn = get_value_precip\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown variable: {variable}\")\n",
    "\n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {n_params:,} trainable parameters\")\n",
    "\n",
    "    return model, loss_fn, get_value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1/5...\n",
      "\n",
      "Building TmaxBiasConvCNPElev model...\n",
      "Model has 287,465 trainable parameters\n",
      "Input dimensions: Frozen({'time': 12418, 'latitude': 29, 'longitude': 61})\n",
      "Training\n",
      "Starting epoch: 0\n",
      "Mean absolute error: 7.075270175933838\n",
      "Pearson correlation: 0.9817459881305695\n",
      "Spearman correlation: 0.9822584812109125\n",
      "Epoch 0: train NLL 3.533, test NLL 3.621\n",
      "Starting epoch: 1\n",
      "Mean absolute error: 7.221829175949097\n",
      "Pearson correlation: 0.9816430509090424\n",
      "Spearman correlation: 0.9822282899759776\n",
      "Epoch 1: train NLL 3.589, test NLL 3.603\n",
      "Starting epoch: 2\n",
      "Mean absolute error: 7.085928916931152\n",
      "Pearson correlation: 0.9817286133766174\n",
      "Spearman correlation: 0.9822368512796202\n",
      "Epoch 2: train NLL 3.688, test NLL 3.617\n",
      "Starting epoch: 3\n",
      "Mean absolute error: 7.2109925746917725\n",
      "Pearson correlation: 0.9818104803562164\n",
      "Spearman correlation: 0.9823026576439161\n",
      "Epoch 3: train NLL 3.644, test NLL 3.601\n",
      "Starting epoch: 4\n",
      "Mean absolute error: 7.140432119369507\n",
      "Pearson correlation: 0.981837272644043\n",
      "Spearman correlation: 0.9823055562638611\n",
      "Epoch 4: train NLL 3.719, test NLL 3.760\n",
      "Starting epoch: 5\n",
      "Mean absolute error: 7.366592645645142\n",
      "Pearson correlation: 0.9816910922527313\n",
      "Spearman correlation: 0.9822196253741915\n",
      "Epoch 5: train NLL 3.646, test NLL 3.610\n",
      "Starting epoch: 6\n",
      "Mean absolute error: 7.015515327453613\n",
      "Pearson correlation: 0.9816616475582123\n",
      "Spearman correlation: 0.9822366541770727\n",
      "Epoch 6: train NLL 3.735, test NLL 3.651\n",
      "Starting epoch: 7\n",
      "Mean absolute error: 7.118581056594849\n",
      "Pearson correlation: 0.9817221462726593\n",
      "Spearman correlation: 0.9822011204386594\n",
      "Epoch 7: train NLL 3.718, test NLL 3.585\n",
      "Starting epoch: 8\n",
      "Mean absolute error: 7.224198579788208\n",
      "Pearson correlation: 0.9815986752510071\n",
      "Spearman correlation: 0.9821422225669378\n",
      "Epoch 8: train NLL 3.618, test NLL 3.589\n",
      "Starting epoch: 9\n",
      "Mean absolute error: 7.051229953765869\n",
      "Pearson correlation: 0.9815877974033356\n",
      "Spearman correlation: 0.9820842493672046\n",
      "Epoch 9: train NLL 3.603, test NLL 3.578\n",
      "Starting epoch: 10\n",
      "Mean absolute error: 6.9484148025512695\n",
      "Pearson correlation: 0.981410562992096\n",
      "Spearman correlation: 0.9820813465023039\n",
      "Epoch 10: train NLL 3.631, test NLL 3.607\n",
      "Starting epoch: 11\n",
      "Mean absolute error: 7.594785690307617\n",
      "Pearson correlation: 0.9801293909549713\n",
      "Spearman correlation: 0.9808468390147407\n",
      "Epoch 11: train NLL 3.559, test NLL 3.701\n",
      "Starting epoch: 12\n",
      "Mean absolute error: 6.982719421386719\n",
      "Pearson correlation: 0.980018675327301\n",
      "Spearman correlation: 0.9809733248320789\n",
      "Epoch 12: train NLL 3.671, test NLL 3.586\n",
      "Starting epoch: 13\n",
      "Mean absolute error: 7.251939296722412\n",
      "Pearson correlation: 0.9447140991687775\n",
      "Spearman correlation: 0.9469079205467348\n",
      "Epoch 13: train NLL 3.541, test NLL 3.599\n",
      "Starting epoch: 14\n",
      "Mean absolute error: 6.9222187995910645\n",
      "Pearson correlation: 0.9183870851993561\n",
      "Spearman correlation: 0.9225155711280182\n",
      "Epoch 14: train NLL 3.634, test NLL 3.593\n",
      "Starting epoch: 15\n",
      "Mean absolute error: 7.158150911331177\n",
      "Pearson correlation: 0.8247948884963989\n",
      "Spearman correlation: 0.8326531349142845\n",
      "Epoch 15: train NLL 3.529, test NLL 3.579\n",
      "Starting epoch: 16\n",
      "Mean absolute error: 7.069292068481445\n",
      "Pearson correlation: 0.8408454358577728\n",
      "Spearman correlation: 0.8503528450106144\n",
      "Epoch 16: train NLL 3.589, test NLL 3.564\n",
      "Starting epoch: 17\n",
      "Mean absolute error: 6.8144097328186035\n",
      "Pearson correlation: 0.8526579141616821\n",
      "Spearman correlation: 0.8636160667630374\n",
      "Epoch 17: train NLL 3.709, test NLL 3.575\n",
      "Starting epoch: 18\n",
      "Mean absolute error: 6.7838404178619385\n",
      "Pearson correlation: 0.8310840129852295\n",
      "Spearman correlation: 0.8426622217564629\n",
      "Epoch 18: train NLL 3.409, test NLL 3.529\n",
      "Starting epoch: 19\n",
      "Mean absolute error: 7.015355348587036\n",
      "Pearson correlation: 0.9370489120483398\n",
      "Spearman correlation: 0.9419774726001358\n",
      "Epoch 19: train NLL 3.480, test NLL 3.550\n",
      "Starting epoch: 20\n",
      "Mean absolute error: 8.176156044006348\n",
      "Pearson correlation: -0.05194259062409401\n",
      "Spearman correlation: -0.056112369878377914\n",
      "Epoch 20: train NLL 4.004, test NLL 3.921\n",
      "Starting epoch: 21\n",
      "Mean absolute error: 4.24656867980957\n",
      "Pearson correlation: 0.9700258076190948\n",
      "Spearman correlation: 0.9796991939610363\n",
      "Epoch 21: train NLL 2.900, test NLL 3.010\n",
      "Starting epoch: 22\n",
      "Mean absolute error: 3.120934844017029\n",
      "Pearson correlation: 0.9796993136405945\n",
      "Spearman correlation: 0.9824268889280422\n",
      "Epoch 22: train NLL 2.470, test NLL 2.995\n",
      "Starting epoch: 23\n",
      "Mean absolute error: 3.971874713897705\n",
      "Pearson correlation: 0.979765385389328\n",
      "Spearman correlation: 0.9821919285254475\n",
      "Epoch 23: train NLL 2.283, test NLL 3.705\n",
      "Starting epoch: 24\n",
      "Mean absolute error: 1.6240102648735046\n",
      "Pearson correlation: 0.9817691147327423\n",
      "Spearman correlation: 0.9824836436433643\n",
      "Epoch 24: train NLL 2.444, test NLL 2.224\n",
      "Starting epoch: 25\n",
      "Mean absolute error: 1.382308006286621\n",
      "Pearson correlation: 0.9814614653587341\n",
      "Spearman correlation: 0.9826259243865305\n",
      "Epoch 25: train NLL 2.380, test NLL 2.260\n",
      "Starting epoch: 26\n",
      "Mean absolute error: 1.5000649094581604\n",
      "Pearson correlation: 0.9817718863487244\n",
      "Spearman correlation: 0.9827437675556522\n",
      "Epoch 26: train NLL 2.224, test NLL 2.207\n",
      "Starting epoch: 27\n",
      "Mean absolute error: 2.99700665473938\n",
      "Pearson correlation: 0.9820942580699921\n",
      "Spearman correlation: 0.9830610019310425\n",
      "Epoch 27: train NLL 2.274, test NLL 3.027\n",
      "Starting epoch: 28\n",
      "Mean absolute error: 1.8762399554252625\n",
      "Pearson correlation: 0.9817297756671906\n",
      "Spearman correlation: 0.982833724647268\n",
      "Epoch 28: train NLL 2.402, test NLL 2.436\n",
      "Starting epoch: 29\n",
      "Mean absolute error: 1.5928857326507568\n",
      "Pearson correlation: 0.9825773537158966\n",
      "Spearman correlation: 0.9831413865774109\n",
      "Epoch 29: train NLL 2.183, test NLL 2.187\n",
      "Starting epoch: 30\n",
      "Mean absolute error: 1.7125794291496277\n",
      "Pearson correlation: 0.9825457334518433\n",
      "Spearman correlation: 0.9833200794310474\n",
      "Epoch 30: train NLL 2.271, test NLL 2.260\n",
      "Starting epoch: 31\n",
      "Mean absolute error: 2.253592848777771\n",
      "Pearson correlation: 0.9828530251979828\n",
      "Spearman correlation: 0.9834297477704042\n",
      "Epoch 31: train NLL 2.344, test NLL 2.495\n",
      "Starting epoch: 32\n",
      "Mean absolute error: 2.636215329170227\n",
      "Pearson correlation: 0.9826479554176331\n",
      "Spearman correlation: 0.9833830927663154\n",
      "Epoch 32: train NLL 2.197, test NLL 2.934\n",
      "Starting epoch: 33\n",
      "Mean absolute error: 1.3885438442230225\n",
      "Pearson correlation: 0.9830082058906555\n",
      "Spearman correlation: 0.9836689208596997\n",
      "Epoch 33: train NLL 2.353, test NLL 2.096\n",
      "Starting epoch: 34\n",
      "Mean absolute error: 1.3095304369926453\n",
      "Pearson correlation: 0.9833000600337982\n",
      "Spearman correlation: 0.9838822276162744\n",
      "Epoch 34: train NLL 2.228, test NLL 2.158\n",
      "Starting epoch: 35\n",
      "Mean absolute error: 2.807065486907959\n",
      "Pearson correlation: 0.9834388792514801\n",
      "Spearman correlation: 0.9839689969116965\n",
      "Epoch 35: train NLL 2.269, test NLL 2.868\n",
      "Starting epoch: 36\n",
      "Mean absolute error: 1.3079556226730347\n",
      "Pearson correlation: 0.9836306869983673\n",
      "Spearman correlation: 0.9842101395746035\n",
      "Epoch 36: train NLL 2.124, test NLL 2.063\n",
      "Starting epoch: 37\n",
      "Mean absolute error: 2.650314211845398\n",
      "Pearson correlation: 0.9829902350902557\n",
      "Spearman correlation: 0.9840989065060481\n",
      "Epoch 37: train NLL 2.262, test NLL 2.706\n",
      "Starting epoch: 38\n",
      "Mean absolute error: 2.134100079536438\n",
      "Pearson correlation: 0.983635663986206\n",
      "Spearman correlation: 0.9842865707186854\n",
      "Epoch 38: train NLL 2.272, test NLL 2.429\n",
      "Starting epoch: 39\n",
      "Mean absolute error: 1.412570297718048\n",
      "Pearson correlation: 0.9838358461856842\n",
      "Spearman correlation: 0.9844355984433928\n",
      "Epoch 39: train NLL 2.086, test NLL 2.065\n",
      "Starting epoch: 40\n",
      "Mean absolute error: 2.0061193704605103\n",
      "Pearson correlation: 0.9838191568851471\n",
      "Spearman correlation: 0.984461189973147\n",
      "Epoch 40: train NLL 2.125, test NLL 2.388\n",
      "Starting epoch: 41\n",
      "Mean absolute error: 1.2722400426864624\n",
      "Pearson correlation: 0.9840230047702789\n",
      "Spearman correlation: 0.9845465132860565\n",
      "Epoch 41: train NLL 2.176, test NLL 2.026\n",
      "Starting epoch: 42\n",
      "Mean absolute error: 1.3884679079055786\n",
      "Pearson correlation: 0.9840703308582306\n",
      "Spearman correlation: 0.9846112198318975\n",
      "Epoch 42: train NLL 2.035, test NLL 2.077\n",
      "Starting epoch: 43\n",
      "Mean absolute error: 1.4623677730560303\n",
      "Pearson correlation: 0.9837665855884552\n",
      "Spearman correlation: 0.984630737069787\n",
      "Epoch 43: train NLL 2.302, test NLL 2.300\n",
      "Starting epoch: 44\n",
      "Mean absolute error: 1.599807858467102\n",
      "Pearson correlation: 0.9840330481529236\n",
      "Spearman correlation: 0.9846759041604379\n",
      "Epoch 44: train NLL 2.322, test NLL 2.260\n",
      "Starting epoch: 45\n",
      "Mean absolute error: 1.5393397212028503\n",
      "Pearson correlation: 0.9845632016658783\n",
      "Spearman correlation: 0.984957985596889\n",
      "Epoch 45: train NLL 1.990, test NLL 2.086\n",
      "Starting epoch: 46\n",
      "Mean absolute error: 1.3362345695495605\n",
      "Pearson correlation: 0.984684556722641\n",
      "Spearman correlation: 0.9851250772686109\n",
      "Epoch 46: train NLL 1.983, test NLL 2.033\n",
      "Starting epoch: 47\n",
      "Mean absolute error: 2.715568423271179\n",
      "Pearson correlation: 0.984687328338623\n",
      "Spearman correlation: 0.9851988966501468\n",
      "Epoch 47: train NLL 2.129, test NLL 3.103\n",
      "Starting epoch: 48\n",
      "Mean absolute error: 2.915273904800415\n",
      "Pearson correlation: 0.9845574498176575\n",
      "Spearman correlation: 0.9852407132326626\n",
      "Epoch 48: train NLL 2.245, test NLL 3.174\n",
      "Starting epoch: 49\n",
      "Mean absolute error: 1.3605406880378723\n",
      "Pearson correlation: 0.9849472641944885\n",
      "Spearman correlation: 0.9853535033477319\n",
      "Epoch 49: train NLL 1.957, test NLL 2.019\n",
      "Starting epoch: 50\n",
      "Mean absolute error: 1.6235079765319824\n",
      "Pearson correlation: 0.9849520623683929\n",
      "Spearman correlation: 0.9854058278606428\n",
      "Epoch 50: train NLL 1.910, test NLL 2.226\n",
      "Starting epoch: 51\n",
      "Mean absolute error: 2.0099047422409058\n",
      "Pearson correlation: 0.98506298661232\n",
      "Spearman correlation: 0.9855855390073243\n",
      "Epoch 51: train NLL 2.004, test NLL 2.407\n",
      "Starting epoch: 52\n",
      "Mean absolute error: 1.1693280935287476\n",
      "Pearson correlation: 0.9850437343120575\n",
      "Spearman correlation: 0.9854966774705276\n",
      "Epoch 52: train NLL 2.016, test NLL 1.890\n",
      "Starting epoch: 53\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a flat elevation map\n",
    "# elev = torch.zeros(input_ds.sizes['latitude'], input_ds.sizes['longitude']).to(device)\n",
    "num_target_points = target_ds.sizes['latitude'] * target_ds.sizes['longitude']\n",
    "elev = torch.zeros(num_target_points, 3).to(device)\n",
    "\n",
    "# Loop over cross-validation folds\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f'\\nStarting fold {fold + 1}/{N_FOLDS}...\\n')\n",
    "\n",
    "    # Reset model weights and optimizer for each fold\n",
    "    set_seed(SEED + fold)  # Use a different seed for each fold for robustness\n",
    "    model, loss_fn, get_value_fn = build_model(VARIABLE, N_CHANNELS, N_BLOCKS, KERNEL_SIZE, LENGTH_SCALE, IN_CHANNELS)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Note: fold splitting logic is inside train_elev\n",
    "    print(f'Input dimensions: {input_ds.sizes}')\n",
    "    \n",
    "    train_input_ds = torch.from_numpy(input_ds['t2m_max'].values).unsqueeze(1).to(device)\n",
    "    # Silly way to simulate 25 channels, we will need to properly add channels like in page 9 of the paper\n",
    "    train_input_ds = train_input_ds.repeat(1, IN_CHANNELS, 1, 1).to(device)\n",
    "    train_target_ds = torch.from_numpy(target_ds['t_max'].values).to(device)\n",
    "    # Reshaping from (time, latitude, longitude) to (time, latitude * longitude)\n",
    "    train_target_ds = train_target_ds.reshape(train_target_ds.shape[0], -1)\n",
    "    \n",
    "    # Train the model\n",
    "    train_elev(\n",
    "        model=model,\n",
    "        opt=optimizer,\n",
    "        ll=loss_fn,\n",
    "        elev=elev,\n",
    "        dists=dists,\n",
    "        y_context=train_input_ds,\n",
    "        y_target=train_target_ds,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        y_target_t=None,  # did not figure out what this does, it's not used in Vaughan's code\n",
    "        get_value=get_value_fn,\n",
    "        fold=fold,\n",
    "        n_epochs=N_EPOCHS\n",
    "    )\n",
    "\n",
    "print(f'\\nTraining finished for all {N_FOLDS} folds. Models saved in {output_dir}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
