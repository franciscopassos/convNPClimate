{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import pandas as pd\n",
    "\n",
    "import plotly\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from numpy.random import gamma, binomial\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd \n",
    "import xarray as xr \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.normal import Normal\n",
    "from convCNP.validation.utils import get_dists\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import xarray as xr\n",
    "\n",
    "from convCNP.models.elev_models import TmaxBiasConvCNPElev, GammaBiasConvCNPElev\n",
    "from convCNP.models.cnn import CNN, ResConvBlock\n",
    "from convCNP.training.training_elev import train_elev\n",
    "from convCNP.training.loss_functions import gll, gamma_ll\n",
    "from convCNP.training.utils import get_value_tmax\n",
    "from convCNP.validation.utils import get_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and mode parameters\n",
    "VARIABLE = 'tmax'   # 'tmax' or 'precip'\n",
    "DATA_YEAR_START = 2023  # Year to start loading data from, None to include all data\n",
    "                        # Used for quicker testing\n",
    "\n",
    "# Model parameters\n",
    "N_CHANNELS = 128    # default in paper is 128\n",
    "N_BLOCKS = 6        # default in paper is 6\n",
    "KERNEL_SIZE = 5     # default in paper is 5\n",
    "LENGTH_SCALE = 0.1  # default in paper is 0.1\n",
    "IN_CHANNELS = 25    # default in paper is 25\n",
    "\n",
    "# Training parameters\n",
    "N_EPOCHS = 100      # default in paper is 100\n",
    "BATCH_SIZE = 16     # default in paper is 16    # TODO: wire this in\n",
    "LR = 5e-4           # default in paper is 5e-4\n",
    "PATIENCE = 10       # default in paper is 10    # TODO: wire this in\n",
    "\n",
    "# Cross-validation parameters\n",
    "N_FOLDS = 5         # default in paper is 5\n",
    "\n",
    "# Other parameters\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'trained_models/'\n",
    "\n",
    "# Auto-set data paths based on variable\n",
    "if VARIABLE == 'tmax':\n",
    "    GRID_INPUTS, GRID_TARGETS = f'./datasets/ERA5_Land/max_temperature/*.nc', f'./datasets/EOBS/max_temperature/*.nc'\n",
    "elif VARIABLE == 'precip':\n",
    "    GRID_INPUTS, GRID_TARGETS = f'./datasets/ERA5_Land/precipitation/*.nc', f'./datasets/EOBS/precipitation/*.nc'\n",
    "ELEV_INPUTS = ''  # Empty for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " <xarray.Dataset> Size: 166MB\n",
      "Dimensions:    (time: 23376, latitude: 29, longitude: 61)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 187kB 1960-01-01 1960-01-02 ... 2023-12-31\n",
      "  * latitude   (latitude) float64 232B 48.2 48.1 48.0 47.9 ... 45.6 45.5 45.4\n",
      "  * longitude  (longitude) float64 488B 5.0 5.1 5.2 5.3 ... 10.7 10.8 10.9 11.0\n",
      "Data variables:\n",
      "    t2m_max    (time, latitude, longitude) float32 165MB dask.array<chunksize=(366, 29, 61), meta=np.ndarray>\n",
      "\n",
      "Target data:\n",
      " <xarray.Dataset> Size: 122MB\n",
      "Dimensions:    (time: 19540, latitude: 30, longitude: 52)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 156kB 1971-01-01 1971-01-02 ... 2024-06-30\n",
      "  * latitude   (latitude) float64 240B 45.35 45.45 45.55 ... 48.05 48.15 48.25\n",
      "  * longitude  (longitude) float64 416B 5.65 5.75 5.85 ... 10.55 10.65 10.75\n",
      "Data variables:\n",
      "    t_max      (time, latitude, longitude) float32 122MB dask.array<chunksize=(1, 30, 52), meta=np.ndarray>\n",
      "Attributes:\n",
      "    CDI:            Climate Data Interface version 1.9.6 (http://mpimet.mpg.d...\n",
      "    history:        Tue Jul 22 14:45:22 2025: cdo -chname,tx,t_max -sellonlat...\n",
      "    Conventions:    CF-1.4\n",
      "    E-OBS_version:  30.0e\n",
      "    References:     http://surfobs.climate.copernicus.eu/dataaccess/access_eo...\n",
      "    NCO:            netCDF Operators version 5.1.8 (Homepage = http://nco.sf....\n",
      "    CDO:            Climate Data Operators version 1.9.6 (http://mpimet.mpg.d...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "full_input_ds = xr.open_mfdataset(GRID_INPUTS, combine='by_coords')\n",
    "full_target_ds = xr.open_mfdataset(GRID_TARGETS, combine='by_coords')\n",
    "\n",
    "print(\"Input data:\\n\", full_input_ds)\n",
    "print(\"\\nTarget data:\\n\", full_target_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data xarray:\n",
      " <xarray.Dataset> Size: 88MB\n",
      "Dimensions:    (time: 12418, latitude: 29, longitude: 61)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 99kB 1990-01-01 1990-01-02 ... 2023-12-31\n",
      "  * latitude   (latitude) float64 232B 48.2 48.1 48.0 47.9 ... 45.6 45.5 45.4\n",
      "  * longitude  (longitude) float64 488B 5.0 5.1 5.2 5.3 ... 10.7 10.8 10.9 11.0\n",
      "Data variables:\n",
      "    t2m_max    (time, latitude, longitude) float32 88MB dask.array<chunksize=(365, 29, 61), meta=np.ndarray>\n",
      "\n",
      "Target data xarray:\n",
      " <xarray.Dataset> Size: 79MB\n",
      "Dimensions:    (time: 12600, latitude: 30, longitude: 52)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 101kB 1990-01-01 1990-01-02 ... 2024-06-30\n",
      "  * latitude   (latitude) float64 240B 45.35 45.45 45.55 ... 48.05 48.15 48.25\n",
      "  * longitude  (longitude) float64 416B 5.65 5.75 5.85 ... 10.55 10.65 10.75\n",
      "Data variables:\n",
      "    t_max      (time, latitude, longitude) float32 79MB dask.array<chunksize=(1, 30, 52), meta=np.ndarray>\n",
      "Attributes:\n",
      "    CDI:            Climate Data Interface version 1.9.6 (http://mpimet.mpg.d...\n",
      "    history:        Tue Jul 22 14:45:22 2025: cdo -chname,tx,t_max -sellonlat...\n",
      "    Conventions:    CF-1.4\n",
      "    E-OBS_version:  30.0e\n",
      "    References:     http://surfobs.climate.copernicus.eu/dataaccess/access_eo...\n",
      "    NCO:            netCDF Operators version 5.1.8 (Homepage = http://nco.sf....\n",
      "    CDO:            Climate Data Operators version 1.9.6 (http://mpimet.mpg.d...\n"
     ]
    }
   ],
   "source": [
    "# Select data from YEAR_START onwards\n",
    "if DATA_YEAR_START is not None:\n",
    "    input_ds = full_input_ds.sel(time=slice(str(DATA_YEAR_START), None))\n",
    "    target_ds = full_target_ds.sel(time=slice(str(DATA_YEAR_START), None))\n",
    "else:\n",
    "    input_ds = full_input_ds\n",
    "    target_ds = full_target_ds\n",
    "\n",
    "print(\"Input data xarray:\\n\", input_ds)\n",
    "print(\"\\nTarget data xarray:\\n\", target_ds)\n",
    "\n",
    "# create a torch tensor with dimensions (time, lat, long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dists(era5_inputs, eobs_targets):\n",
    "    \"\"\"\n",
    "    Get the distances between the grid points and true points.\n",
    "    \"\"\"\n",
    "    era5_long_grid, era5_lat_grid = np.meshgrid(era5_inputs['longitude'], era5_inputs['latitude'])\n",
    "    era5_lat_grid = torch.from_numpy(era5_lat_grid).to(device)\n",
    "    era5_long_grid = torch.from_numpy(era5_long_grid).to(device)\n",
    "\n",
    "    eobs_coords = eobs_targets.stack(coords=['latitude', 'longitude'])['coords'].values\n",
    "\n",
    "    return get_dists(eobs_coords, era5_lat_grid, era5_long_grid)\n",
    "\n",
    "dists = calculate_dists(input_ds, target_ds).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(variable, n_channels, n_blocks, kernel_size, length_scale, in_channels):\n",
    "    \"\"\"\n",
    "    Build convCNP model based on variable type\n",
    "\n",
    "    Returns:\n",
    "        model: nn.Module\n",
    "        loss_fn: function\n",
    "        get_value_fn: function to extract predictions from model output\n",
    "    \"\"\"\n",
    "    # Build CNN decoder (same architecture for both variables)\n",
    "    decoder = CNN(\n",
    "        n_channels=n_channels,\n",
    "        ConvBlock=ResConvBlock,\n",
    "        n_blocks=n_blocks,\n",
    "        Conv=nn.Conv2d,\n",
    "        Normalization=nn.Identity,\n",
    "        kernel_size=kernel_size\n",
    "    )\n",
    "\n",
    "    # Build model based on variable\n",
    "    if variable == 'tmax':\n",
    "        print(\"Building TmaxBiasConvCNPElev model...\")\n",
    "        model = TmaxBiasConvCNPElev(\n",
    "            decoder=decoder,\n",
    "            in_channels=in_channels,\n",
    "            ls=length_scale\n",
    "        )\n",
    "        loss_fn = gll\n",
    "        get_value_fn = get_value_tmax\n",
    "\n",
    "    elif variable == 'precip':\n",
    "        print(\"Building GammaBiasConvCNPElev model...\")\n",
    "        model = GammaBiasConvCNPElev(\n",
    "            decoder=decoder,\n",
    "            in_channels=in_channels,\n",
    "            ls=length_scale\n",
    "        )\n",
    "        loss_fn = gamma_ll\n",
    "        # For precipitation, we need a custom function\n",
    "        def get_value_precip(p):\n",
    "            \"\"\"Extract mean prediction for precipitation\"\"\"\n",
    "            # For Gamma distribution, mean = alpha / beta\n",
    "            # Only predict non-zero when rho > 0.5\n",
    "            mean = p[:, :, 1] / p[:, :, 2]  # alpha / beta\n",
    "            mean[p[:, :, 0] <= 0.5] = 0  # Set to 0 when dry\n",
    "            return mean\n",
    "        get_value_fn = get_value_precip\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown variable: {variable}\")\n",
    "\n",
    "    # Count parameters\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Model has {n_params:,} trainable parameters\")\n",
    "\n",
    "    return model, loss_fn, get_value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fold 1/5...\n",
      "\n",
      "Building TmaxBiasConvCNPElev model...\n",
      "Model has 287,465 trainable parameters\n",
      "Input dimensions: Frozen({'time': 12418, 'latitude': 29, 'longitude': 61})\n",
      "Training\n",
      "Starting epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 1560 but got size 29 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[136]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     train_target_ds = torch.from_numpy(target_ds[\u001b[33m'\u001b[39m\u001b[33mt_max\u001b[39m\u001b[33m'\u001b[39m].values).to(device)\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     \u001b[43mtrain_elev\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mll\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43melev\u001b[49m\u001b[43m=\u001b[49m\u001b[43melev\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdists\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_input_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_target_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m        \u001b[49m\u001b[43my_target_t\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# did not figure out what this does, it's not used in Vaughan's code\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m        \u001b[49m\u001b[43mget_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_value_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_EPOCHS\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining finished for all \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_FOLDS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m folds. Models saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/convNPClimate/convCNP/training/training_elev.py:138\u001b[39m, in \u001b[36mtrain_elev\u001b[39m\u001b[34m(model, opt, ll, elev, dists, y_context, y_target, output_dir, y_target_t, get_value, fold, n_epochs)\u001b[39m\n\u001b[32m    135\u001b[39m     del held_out\n\u001b[32m    136\u001b[39m training_data, held_out = get_fold_data((8766, 10958), y_context, y_target)\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m # Compute training objective.\n\u001b[32m    139\u001b[39m train_obj = train_epoch_elev(model, opt, training_data, ll, elev, dists)\n\u001b[32m    140\u001b[39m test_obj = eval_epoch_elev(model, held_out, ll, elev, dists, y_target_t, get_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/convNPClimate/convCNP/training/training_elev.py:102\u001b[39m, in \u001b[36mtrain_epoch_elev\u001b[39m\u001b[34m(model, opt, training_data, ll, elev, dists)\u001b[39m\n\u001b[32m     99\u001b[39m # Train and update the model\n\u001b[32m    100\u001b[39m batch_objs = []\n\u001b[32m    101\u001b[39m for task in training_data:\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     # Generate a mask\n\u001b[32m    103\u001b[39m     obj, opt, model = train_batch_elev(task, opt, model, ll, elev, dists)\n\u001b[32m    104\u001b[39m     batch_objs.append(float(obj.item()))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/convNPClimate/convCNP/training/training_elev.py:27\u001b[39m, in \u001b[36mtrain_batch_elev\u001b[39m\u001b[34m(task, opt, model, ll, elev, dists)\u001b[39m\n\u001b[32m     24\u001b[39m # Generate mask\n\u001b[32m     25\u001b[39m mask = generate_context_mask(batch_size, channels, x, y)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m # Forward pass  \n\u001b[32m     28\u001b[39m v = model(task['y_context'], mask, dists, elev)\n\u001b[32m     30\u001b[39m # Backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/capstone/lib/python3.13/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/capstone/lib/python3.13/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/convNPClimate/convCNP/models/elev_models.py:62\u001b[39m, in \u001b[36mTmaxBiasConvCNPElev.forward\u001b[39m\u001b[34m(self, x, mask, dists, elev)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Do elevation\u001b[39;00m\n\u001b[32m     61\u001b[39m elev = elev.repeat(out.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m out = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melev\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m out = \u001b[38;5;28mself\u001b[39m.elev_mlp(out)\n\u001b[32m     64\u001b[39m out[...,\u001b[32m1\u001b[39m] = force_positive(out[...,\u001b[32m1\u001b[39m])\n",
      "\u001b[31mRuntimeError\u001b[39m: Sizes of tensors must match except in dimension 2. Expected size 1560 but got size 29 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "output_dir = Path(OUTPUT_DIR)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create a flat elevation map\n",
    "elev = torch.zeros(input_ds.sizes['latitude'], input_ds.sizes['longitude']).to(device)\n",
    "\n",
    "# Loop over cross-validation folds\n",
    "for fold in range(N_FOLDS):\n",
    "    print(f'\\nStarting fold {fold + 1}/{N_FOLDS}...\\n')\n",
    "\n",
    "    # Reset model weights and optimizer for each fold\n",
    "    set_seed(SEED + fold)  # Use a different seed for each fold for robustness\n",
    "    model, loss_fn, get_value_fn = build_model(VARIABLE, N_CHANNELS, N_BLOCKS, KERNEL_SIZE, LENGTH_SCALE, IN_CHANNELS)\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    # Note: fold splitting logic is inside train_elev\n",
    "    print(f'Input dimensions: {input_ds.sizes}')\n",
    "    \n",
    "    train_input_ds = torch.from_numpy(input_ds['t2m_max'].values).unsqueeze(1).to(device)\n",
    "    # Silly way to simulate 25 channels, we will need to properly add channels like in page 9 of the paper\n",
    "    train_input_ds = train_input_ds.repeat(1, IN_CHANNELS, 1, 1).to(device)\n",
    "    train_target_ds = torch.from_numpy(target_ds['t_max'].values).to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    train_elev(\n",
    "        model=model,\n",
    "        opt=optimizer,\n",
    "        ll=loss_fn,\n",
    "        elev=elev,\n",
    "        dists=dists,\n",
    "        y_context=train_input_ds,\n",
    "        y_target=train_target_ds,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        y_target_t=None,  # did not figure out what this does, it's not used in Vaughan's code\n",
    "        get_value=get_value_fn,\n",
    "        fold=fold,\n",
    "        n_epochs=N_EPOCHS\n",
    "    )\n",
    "\n",
    "print(f'\\nTraining finished for all {N_FOLDS} folds. Models saved in {output_dir}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
